# 岭回归 #

回归算法是最常用的算法之一。回归算法的形式十分简单，它期望使用一个超平面来拟合数据集。如果数据集中的变量存在线性关系，那么其就能拟合地非常好。在实际应用中，简单的线性回归常常被正则化的回归方法（Ridge、LASSO和Elastic-Net）所代替。正则化其实是为了避免过拟合的一种技术手段，采取惩罚项以降低模型过拟合的风险。我们今天所介绍的岭回归(Ridge Regression)是一种针对普通最小二乘法所存在的不足进行的一种改进，是在平方误差的基础上增加正则化项，即增加了L2惩罚项。所以岭回归就是一种带二范数惩罚的最小二乘回归。

一般的回归模型很容易发生过拟合的问题。我们首先来看多项式回归的过拟合现象。理论上，只要多项式的阶数足够高，多项式就能够以任意精度来逼近我们的训练数据，只是此时，往往伴随而来的就是过拟合了。

可以看出，多项式回归为了更好地拟合我们的训练数据，生成的模型往往伴随着非常大的回归系数，而这些系数大到使得模型的可解释性非常弱。

样本数量很少时，随着模型复杂度的增加，很容易发生过拟合，此时回归得不到有意义的结果，然而当我们有足够多的观察样本时，由于此时我们的训练数据可以很好地代表真实的世界，回归会有较小的方差，则很难发生过拟合。

我们由普通最小二乘法所生成的模型，为了尽可能地去拟合训练样本，而产生的模型带有很大数量级的回归参数，而这种模型往往会在测试集上表现的很糟糕。我们能否去解决这个棘手的问题呢？现在我们进一步讨论。

首先我们先试图建立一个用来衡量模型好坏的损失函数。

对于回归问题来说，我们来衡量模型的好坏主要表现在模型精度和模型的解释能力两个方面。关于这个损失函数，一方面，我们要尽可能的拟合训练数据，另一方面，我们又想得到具有一些较小数量级的参数，没有发生过拟合的模型。因此总的损失函数（total cost）可以分为两部分，即

**total cost = measure of fit + measure of magnitude of coefficients**.

首先，关于衡量对训练数据拟合程度的指标，我们可以回顾最小二乘法当中的RSS(Residual Sum of Squares)指标。当RSS很小时，则说明模型拟合的非常好。

其次，关于衡量回归系数量级的指标我们应该如何去定义呢？

第一种形式，系数求和。假若\\({w_0}\\)=554845，\\({w_1}\\) =-556253,而此时\\({w_0} + {w_1}\\) =small。显然这不是一种理想的方式。

第二种形式，绝对值求和，也就是常说的1范数，这种形式看起来很好，不过我们要在LASSO回归中讨论这种形式的利用。

第三种形式，平方和形式，也就是常说的2范数。这种形式可以很好的刻画这个尺度。

在岭回归中我们选用第三种形式来度量回归系数的量级。

至此，我们可以明确的定义总误差（total cost），一共包含两部分。即我们需要考虑最小化的损失函数为 
\\( RSS({\bf{w}}) + \lambda \left\| {\bf{w}} \right\|_2^2 \\)。

在这里， \\(\lambda \\)是我们需要调整的一个参数，叫做岭参数，通过调整\\(\lambda \\) 的大小，用来平衡代价函数中两个指标的尺度。惩罚函数的系数 \\(\lambda \\)越大，目标函数中惩罚项所占的重要性就越高。

如果\\(\lambda \\) 等于0，此时的损失函数就是最小二乘法的形式，所对应的解就是最小二乘法的解 \\( {{\bf{\hat w}}^{LS}} \\) ；

如果 \\(\lambda \\)为无穷大，当 \\( {\bf{\hat w}} \\) 不等于0时，则总误差达到无穷大，

&emsp;&emsp;&emsp;&emsp;当 \\( {\bf{\hat w}} \\) 等于0时，此时total cost =RSS(0),解就是\\( {\bf{\hat w}} \\)  =0；

如果 \\(\lambda \\)介于0和无穷大之间，则有 \\(0 \le \left\| {{\bf{\hat w}}} \right\|_2^2 \le \left\| {{{{\bf{\hat w}}}^{LS}}} \right\|_2^2\\)。


上面的这个形式就是**岭回归**了，也叫做**L2正则化**。
到这里，我们已经完整的介绍了岭回归目标函数的形式。我们可以看出，岭回归是最小二乘法的一种改进，为了解决过拟合的问题，而增加了L2惩罚项。通常，它的原理是牺牲最小二乘法解的无偏性来换取数值解的稳定性，降低精度为代价，使获得回归系数更为符合实际、更可靠。

我们简单回顾一下模型的方差和偏差。模型的偏差指的是模型预测值和数据之间的差异，而方差指的是模型之间的差异。岭回归中我们需要通过不断调整 \\(\lambda \\)，确定惩罚强度，来权衡模型的方差和偏差，以让模型在欠拟合和过拟合之间达到平衡。

对于岭回归的损失函数形式，

当  \\(\lambda \\)较大时，模型具有较高的偏差，而较小的方差；

当  \\(\lambda \\)较小时，模型具有较小的偏差，而较高的方差，例如当 \\(\lambda \\) =0时，就相当于最小二乘法。

本质上，  \\(\lambda \\)控制了模型的复杂度，岭回归的意义就是通过控制 \\(\lambda \\) ，牺牲模型的精确度，而增加模型的泛化能力。通过调整 \\(\lambda \\) 的值可以在方差和偏差之间得到权衡。我们先来看对于给定  \\(\lambda \\)值的岭回归模型。

第一步，为了记法简洁，我们用矩阵符号重写损失函数total cost。

利用最小二乘法中的记法
$$\begin{array}{c}
RSS({\bf{w}}) = \sum\limits_i {({y_i} - h{{({x_i})}^T}{\bf{w}})} \\
 = {({\bf{y}} - {\bf{Hw}})^T}({\bf{y}} - {\bf{Hw}})
\end{array}$$

回归系数的二范数

$$ \begin{array}{c}
\left\| {\bf{w}} \right\|_2^2 = w_0^2 + w_1^2 + w_2^2 +  \cdots  + w_D^2\\
 = {{\bf{w}}^T}{\bf{w}}
\end{array}$$

于是，岭回归的损失函数可以写为

$$ \begin{array}{c}
RSS({\bf{w}}) + \lambda \left\| {\bf{w}} \right\|_2^2\\
 = {({\bf{y}} - {\bf{Hw}})^T}({\bf{y}} - {\bf{Hw}}) + \lambda {{\bf{w}}^T}{\bf{w}}
\end{array}$$

第二步，对于优化问题,我们最常用的就是梯度下降法。
求岭回归代价函数的梯度

$$\begin{array}{c}
\nabla RSS({\bf{w}}) + \lambda \left\| {\bf{w}} \right\|_2^2\\
 = \nabla {({\bf{y}} - {\bf{Hw}})^T}({\bf{y}} - {\bf{Hw}}) + \lambda {{\bf{w}}^T}{\bf{w}}\\
 =  - 2{{\bf{H}}^T}({\bf{y}} - {\bf{Hw}}) + 2\lambda {\bf{w}}
\end{array}$$

第三步，我们可以有两种实现方式。

其一，令梯度直接等于0，

$$\nabla {\mathop{\rm cost}\nolimits} ({\bf{w}}) =  - 2{{\bf{H}}^T}({\bf{y}} - {\bf{Hw}}) + 2\lambda {\bf{w}} = 0$$

解得 \\({{\bf{\hat w}}^{ridge}} = {({{\bf{H}}^T}{\bf{H}} + \lambda {\bf{I}})^{ - 1}}{{\bf{H}}^T}{\bf{y}}\\)


在这里，我们从代数的角度，简要的分析一下岭回归所做的事情。

在回归分析中最常用的方法是最小二乘法，而使用最小二乘法的一个前提是 \\(|{{\bf{H}}^T}{\bf{H}}|\\)不为零，即矩阵 \\({{\bf{H}}^T}{\bf{H}}\\)非奇异，当各个特征之间有较强的线性相关性时，矩阵\\({{\bf{H}}^T}{\bf{H}}\\) 的行列式比较小，甚至趋近于0，此时这个矩阵不能求逆，这种病态矩阵在计算过程中容易造成约数误差，很不稳定，在具体取值上与真实值有较大的偏差，因此得到的数据往往缺乏稳定性和可靠性。而岭回归通过引入  \\(\lambda \\)参数，使得该问题得到解决。岭回归通过增加惩罚项，实际上是在矩阵\\({{\bf{H}}^T}{\bf{H}}\\) 的主对角线元素上人为地增加一个非负因子，修复矩阵达到较好的效果。


基于这个特点，对于多元回归来说，岭回归可用来解决数据的共线性问题。

其二，我们用另一种常见的方法求解
\\({\bf{w}}\\) ，也就是梯度下降法，不断的向最优解移动。

根据损失函数的梯度 
\\(\nabla {\mathop{\rm cost}\nolimits} ({\bf{w}}) =  - 2{{\bf{H}}^T}({\bf{y}} - {\bf{Hw}}) + 2\lambda {\bf{w}}\\)

对于第 \\( j\\)个特征的权重 \\( w_j\\)使用更新公式

$$ 
w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta *[ - 2\sum\limits_{i = 1}^N {{h_j}({x_i})({y_i} - {{\hat y}_i}({{\bf{w}}^{(t)}})) + 2\lambda w_j^{(t)}}]$$

$$ \leftarrow (1 - 2\eta \lambda )w_j^{(t)} + 2\eta \sum\limits_{i = 1}^N {{h_j}({x_i})({y_i} - {{\hat y}_i}({{\bf{w}}^{(t)}}))} $$

\\(\eta \\)为学习率。综上，我们可以得到岭回归算法的完整流程。

接着我们来讨论另一个重要的问题。也就是我们应该如何确定岭回归中的参数 \\(\lambda \\)呢？

![](http://i.imgur.com/UYiZeoq.png)


我们绘制出岭参数 \\(\lambda \\)和回归系数之间的关系图，叫做岭迹图。当 \\(\lambda \\)=0时，也就是最小二乘法的结果。我们可以看到随着 \\(\lambda \\)的增大，回归系数的绝对值在不断的减小；当 \\(\lambda \\)趋于无穷大时，\\({\bf{w}}\\) 将趋于0。

岭参数选择的一般原则是使我们的模型中不存在明显不合常理的回归参数，其岭估计的符号应当要变得合理，回归系数没有不合实际意义的绝对值。

假如我们有足够多的数据时，我们可以按照合理的比例把数据分为训练集、验证集和测试集三部分。在训练集上拟合出\\({\hat w_\lambda }\\) ,在验证集上选择最优的\\(\lambda \\) 值\\({\lambda ^ * }\\) ，进一步在测试集进行评估和预测。

不幸的是，当我们只有一个很小的数据集时，这种方法还能使用吗？

在这种情况下，我们仍然需要留出一小部分数据来作为测验集，问题的关键就是我们该如何利用剩下的数据去找到一个不错的\\(\lambda \\) 呢？

如果我们仍然按照之前的方式，把这部分数据分为训练集和验证集，此时，验证集只包含了很小一部分数据，显然不足以保证我们能选择到一个不错的 \\(\lambda \\)。针对这个问题，下面我们将学习到一个关键的技术，**K折交叉验证**。

k折交叉验证的思想就是，我们把余下的数据随机的分为K组，之后把每一组都当作验证集，其他组的数据当作训练集去拟合出一个模型，用这个模型在验证集上得到相应的误差\\(erro{r_i}\\),依次往下进行，直到最后我们就得到了k个误差值\\(erro{r_i}(\lambda )(i = 1,2, \cdots ,K)\\) 。
我们把所有的误差的平均值 
\\(CV(\lambda ) = \frac{1}{K}\sum\limits_{k = 1}^K {erro{r_k}(\lambda )}\\)当作此时我们指定的\\(\lambda \\) 所对应的误差值。


至此，对于每一个 \\(\lambda \\)值都重复此进程，我们画出岭参数\\(\lambda \\) 和平均误差之间的变化关系，即可选择出最优的 \\(\lambda \\)值。


当然，利用k折交叉验证集，不可避免的就要确定k的取值。

一般的，最好的近似就是把每一个样本点单独作为验证集，也就是留一交叉验证法（leave-one-out cross validation）。但是，此时对于每个 \\(\lambda \\)，需要作N次拟合，需要花费的大量的时间，消耗巨大的计算资源。

经验上，k的取值常常取为5或者10，也就是5折交叉验证和10折交叉验证集。

接下来，我们通过一个简单的数据集演示，来实现岭回归的一个简单实现。
